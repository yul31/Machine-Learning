{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c8091e",
   "metadata": {},
   "source": [
    "### 1. 목표\n",
    "- 보스턴에 있는 주택들의 데이터를 바탕으로 집 가격을 예측해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73835f37",
   "metadata": {},
   "source": [
    "### 2. 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f62bf248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e19714b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#warning 안뜨게 하기\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "678386b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13c36d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "187f125c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston['target'] #정답값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49c492e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston['feature_names']\n",
    "#INDUS: 비상업지구의 비율, CHAS: 강의 경계에 있으면0/없으면1, AGE:1940년 이전에 지어진 주택 비율, RAD: 고속도로 접근 용이성, PTRATIO: 지역 교사와 학생수의 비율, LSTAT: 하위 계층의 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1f001",
   "metadata": {},
   "source": [
    "### 3. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2696b8",
   "metadata": {},
   "source": [
    "### 4. 탐색적 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7797ced8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "0       15.3  396.90   4.98  \n",
       "1       17.8  396.90   9.14  \n",
       "2       17.8  392.83   4.03  \n",
       "3       18.7  394.63   2.94  \n",
       "4       18.7  396.90   5.33  \n",
       "..       ...     ...    ...  \n",
       "501     21.0  391.99   9.67  \n",
       "502     21.0  396.90   9.08  \n",
       "503     21.0  396.90   5.64  \n",
       "504     21.0  393.45   6.48  \n",
       "505     21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.DataFrame(boston['data'], columns = boston['feature_names'])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c1ef073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     price\n",
       "0     24.0\n",
       "1     21.6\n",
       "2     34.7\n",
       "3     33.4\n",
       "4     36.2\n",
       "..     ...\n",
       "501   22.4\n",
       "502   20.6\n",
       "503   23.9\n",
       "504   22.0\n",
       "505   11.9\n",
       "\n",
       "[506 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.DataFrame(boston['target'], columns = ['price'])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "944629b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 13 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     506 non-null    float64\n",
      " 1   ZN       506 non-null    float64\n",
      " 2   INDUS    506 non-null    float64\n",
      " 3   CHAS     506 non-null    float64\n",
      " 4   NOX      506 non-null    float64\n",
      " 5   RM       506 non-null    float64\n",
      " 6   AGE      506 non-null    float64\n",
      " 7   DIS      506 non-null    float64\n",
      " 8   RAD      506 non-null    float64\n",
      " 9   TAX      506 non-null    float64\n",
      " 10  PTRATIO  506 non-null    float64\n",
      " 11  B        506 non-null    float64\n",
      " 12  LSTAT    506 non-null    float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 51.5 KB\n"
     ]
    }
   ],
   "source": [
    "x.info()\n",
    "#506개의 행, 결측치 없음, 실수값으로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeec1590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   price   506 non-null    float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 4.1 KB\n"
     ]
    }
   ],
   "source": [
    "y.info()\n",
    "#506개의 행, 결측치 없음, 실수값으로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8f6c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2666913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "261555e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n",
    "#404개의 행, 13개의 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a5fec1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape\n",
    "#404개의 행, 1개의 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e94da90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 13)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape\n",
    "#102개의 행, 13개의 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dcc5d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape\n",
    "#102개의 행, 1개의 column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f20d51",
   "metadata": {},
   "source": [
    "### 5.모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "379ca17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cda60644",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression() #모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "faff2647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c75441b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.19443447e-01,  4.47799511e-02,  5.48526168e-03,\n",
       "         2.34080361e+00, -1.61236043e+01,  3.70870901e+00,\n",
       "        -3.12108178e-03, -1.38639737e+00,  2.44178327e-01,\n",
       "        -1.09896366e-02, -1.04592119e+00,  8.11010693e-03,\n",
       "        -4.92792725e-01]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_ #가중치 확인 #학습시킨 feature(column) 개수만큼의 가중치 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7966d3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38.09169493])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_ #평균값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de96a7d",
   "metadata": {},
   "source": [
    "### 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76e2b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = lr.predict(x_train)\n",
    "test = lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f59188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = mean_squared_error(train, y_train)\n",
    "test_score = mean_squared_error(test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b51d2033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.326470203585725\n"
     ]
    }
   ],
   "source": [
    "print(train_score) #MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3baf44d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.44897999767653\n"
     ]
    }
   ],
   "source": [
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c54ff4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse : 4.396188144698282\n"
     ]
    }
   ],
   "source": [
    "print('rmse :', train_score**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e270afa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse : 5.783509315085135\n"
     ]
    }
   ],
   "source": [
    "print('rmse :', test_score**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee00f72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.387321170386853\n"
     ]
    }
   ],
   "source": [
    "print(train_score**0.5 - test_score**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64f4a613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7730135569264234"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(x_train, y_train) #학습한 데이터의 r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13959036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5892223849182507"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(x_test, y_test) #새로운 데이터의 r2\n",
    "#두 r2 값의 차가 크므로 좋은 모듈이 아니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e369ff",
   "metadata": {},
   "source": [
    "### 선형회귀 모델 특성확장\n",
    "- 선형회귀는 특성이 적으면 다른 알고리즘에 비해 성능이 낮게 나오기 때문에 특성확장을 사용해서 모델의 복잡도를 증가\n",
    "- 각 컬럼들의 데이터를 곱하여 새로운 특성으로 확장시켜 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aaea7447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy함수 설명\n",
    "#copy함수 사용하지 않을 때\n",
    "a = [1,2,3]\n",
    "b = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f6e9e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0df28762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2a80dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 3]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0] = 0\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07ae9cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 3]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7d7a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy함수 사용하면\n",
    "a = [1,2,3]\n",
    "b = a.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a32a8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 3]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0] = 0\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8633815b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91f3e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_x_train = x_train.copy() #copy함수 사용하지 않으면 원본도 변경됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "abee041e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507</td>\n",
       "      <td>6.951</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>391.70</td>\n",
       "      <td>9.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.15876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413</td>\n",
       "      <td>5.961</td>\n",
       "      <td>17.5</td>\n",
       "      <td>5.2873</td>\n",
       "      <td>4.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>376.94</td>\n",
       "      <td>9.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.11329</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428</td>\n",
       "      <td>6.897</td>\n",
       "      <td>54.3</td>\n",
       "      <td>6.3361</td>\n",
       "      <td>6.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>391.25</td>\n",
       "      <td>11.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>25.94060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.679</td>\n",
       "      <td>5.304</td>\n",
       "      <td>89.1</td>\n",
       "      <td>1.6475</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>127.36</td>\n",
       "      <td>26.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS   RAD    TAX  \\\n",
       "220   0.35809   0.0   6.20   1.0  0.507  6.951  88.5  2.8617   8.0  307.0   \n",
       "71    0.15876   0.0  10.81   0.0  0.413  5.961  17.5  5.2873   4.0  305.0   \n",
       "240   0.11329  30.0   4.93   0.0  0.428  6.897  54.3  6.3361   6.0  300.0   \n",
       "6     0.08829  12.5   7.87   0.0  0.524  6.012  66.6  5.5605   5.0  311.0   \n",
       "417  25.94060   0.0  18.10   0.0  0.679  5.304  89.1  1.6475  24.0  666.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "220     17.4  391.70   9.71  \n",
       "71      19.2  376.94   9.88  \n",
       "240     16.6  391.25  11.38  \n",
       "6       15.2  395.60  12.43  \n",
       "417     20.2  127.36  26.64  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14531bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1997009910.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]\n"
     ]
    }
   ],
   "source": [
    "#extended_x_train의 각 컬럼들을 서로 한번씩 곱해서 새로운 컬럼 추가\n",
    "\n",
    "for col1 in x_train.columns : #13번 반복 (column 13개 이므로)\n",
    "    for col2 in x_train.columns : #13번 반복\n",
    "        extended_x_train[col1 + 'x' + col2] = x_train[col1] * x_train[col2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b232c4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>...</th>\n",
       "      <th>LSTATxCHAS</th>\n",
       "      <th>LSTATxNOX</th>\n",
       "      <th>LSTATxRM</th>\n",
       "      <th>LSTATxAGE</th>\n",
       "      <th>LSTATxDIS</th>\n",
       "      <th>LSTATxRAD</th>\n",
       "      <th>LSTATxTAX</th>\n",
       "      <th>LSTATxPTRATIO</th>\n",
       "      <th>LSTATxB</th>\n",
       "      <th>LSTATxLSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507</td>\n",
       "      <td>6.951</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.71</td>\n",
       "      <td>4.92297</td>\n",
       "      <td>67.49421</td>\n",
       "      <td>859.335</td>\n",
       "      <td>27.787107</td>\n",
       "      <td>77.68</td>\n",
       "      <td>2980.97</td>\n",
       "      <td>168.954</td>\n",
       "      <td>3803.4070</td>\n",
       "      <td>94.2841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.15876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413</td>\n",
       "      <td>5.961</td>\n",
       "      <td>17.5</td>\n",
       "      <td>5.2873</td>\n",
       "      <td>4.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.08044</td>\n",
       "      <td>58.89468</td>\n",
       "      <td>172.900</td>\n",
       "      <td>52.238524</td>\n",
       "      <td>39.52</td>\n",
       "      <td>3013.40</td>\n",
       "      <td>189.696</td>\n",
       "      <td>3724.1672</td>\n",
       "      <td>97.6144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.11329</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428</td>\n",
       "      <td>6.897</td>\n",
       "      <td>54.3</td>\n",
       "      <td>6.3361</td>\n",
       "      <td>6.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.87064</td>\n",
       "      <td>78.48786</td>\n",
       "      <td>617.934</td>\n",
       "      <td>72.104818</td>\n",
       "      <td>68.28</td>\n",
       "      <td>3414.00</td>\n",
       "      <td>188.908</td>\n",
       "      <td>4452.4250</td>\n",
       "      <td>129.5044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.51332</td>\n",
       "      <td>74.72916</td>\n",
       "      <td>827.838</td>\n",
       "      <td>69.117015</td>\n",
       "      <td>62.15</td>\n",
       "      <td>3865.73</td>\n",
       "      <td>188.936</td>\n",
       "      <td>4917.3080</td>\n",
       "      <td>154.5049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>25.94060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.679</td>\n",
       "      <td>5.304</td>\n",
       "      <td>89.1</td>\n",
       "      <td>1.6475</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.08856</td>\n",
       "      <td>141.29856</td>\n",
       "      <td>2373.624</td>\n",
       "      <td>43.889400</td>\n",
       "      <td>639.36</td>\n",
       "      <td>17742.24</td>\n",
       "      <td>538.128</td>\n",
       "      <td>3392.8704</td>\n",
       "      <td>709.6896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS   RAD    TAX  \\\n",
       "220   0.35809   0.0   6.20   1.0  0.507  6.951  88.5  2.8617   8.0  307.0   \n",
       "71    0.15876   0.0  10.81   0.0  0.413  5.961  17.5  5.2873   4.0  305.0   \n",
       "240   0.11329  30.0   4.93   0.0  0.428  6.897  54.3  6.3361   6.0  300.0   \n",
       "6     0.08829  12.5   7.87   0.0  0.524  6.012  66.6  5.5605   5.0  311.0   \n",
       "417  25.94060   0.0  18.10   0.0  0.679  5.304  89.1  1.6475  24.0  666.0   \n",
       "\n",
       "     ...  LSTATxCHAS  LSTATxNOX   LSTATxRM  LSTATxAGE  LSTATxDIS  LSTATxRAD  \\\n",
       "220  ...        9.71    4.92297   67.49421    859.335  27.787107      77.68   \n",
       "71   ...        0.00    4.08044   58.89468    172.900  52.238524      39.52   \n",
       "240  ...        0.00    4.87064   78.48786    617.934  72.104818      68.28   \n",
       "6    ...        0.00    6.51332   74.72916    827.838  69.117015      62.15   \n",
       "417  ...        0.00   18.08856  141.29856   2373.624  43.889400     639.36   \n",
       "\n",
       "     LSTATxTAX  LSTATxPTRATIO    LSTATxB  LSTATxLSTAT  \n",
       "220    2980.97        168.954  3803.4070      94.2841  \n",
       "71     3013.40        189.696  3724.1672      97.6144  \n",
       "240    3414.00        188.908  4452.4250     129.5044  \n",
       "6      3865.73        188.936  4917.3080     154.5049  \n",
       "417   17742.24        538.128  3392.8704     709.6896  \n",
       "\n",
       "[5 rows x 182 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d338da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모든 column 확인하는 방법\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "252b5745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>CRIMxCRIM</th>\n",
       "      <th>CRIMxZN</th>\n",
       "      <th>CRIMxINDUS</th>\n",
       "      <th>CRIMxCHAS</th>\n",
       "      <th>CRIMxNOX</th>\n",
       "      <th>CRIMxRM</th>\n",
       "      <th>CRIMxAGE</th>\n",
       "      <th>CRIMxDIS</th>\n",
       "      <th>CRIMxRAD</th>\n",
       "      <th>CRIMxTAX</th>\n",
       "      <th>CRIMxPTRATIO</th>\n",
       "      <th>CRIMxB</th>\n",
       "      <th>CRIMxLSTAT</th>\n",
       "      <th>ZNxCRIM</th>\n",
       "      <th>ZNxZN</th>\n",
       "      <th>ZNxINDUS</th>\n",
       "      <th>ZNxCHAS</th>\n",
       "      <th>ZNxNOX</th>\n",
       "      <th>ZNxRM</th>\n",
       "      <th>ZNxAGE</th>\n",
       "      <th>ZNxDIS</th>\n",
       "      <th>ZNxRAD</th>\n",
       "      <th>ZNxTAX</th>\n",
       "      <th>ZNxPTRATIO</th>\n",
       "      <th>ZNxB</th>\n",
       "      <th>ZNxLSTAT</th>\n",
       "      <th>INDUSxCRIM</th>\n",
       "      <th>INDUSxZN</th>\n",
       "      <th>INDUSxINDUS</th>\n",
       "      <th>INDUSxCHAS</th>\n",
       "      <th>INDUSxNOX</th>\n",
       "      <th>INDUSxRM</th>\n",
       "      <th>INDUSxAGE</th>\n",
       "      <th>INDUSxDIS</th>\n",
       "      <th>INDUSxRAD</th>\n",
       "      <th>INDUSxTAX</th>\n",
       "      <th>INDUSxPTRATIO</th>\n",
       "      <th>INDUSxB</th>\n",
       "      <th>INDUSxLSTAT</th>\n",
       "      <th>CHASxCRIM</th>\n",
       "      <th>CHASxZN</th>\n",
       "      <th>CHASxINDUS</th>\n",
       "      <th>CHASxCHAS</th>\n",
       "      <th>CHASxNOX</th>\n",
       "      <th>CHASxRM</th>\n",
       "      <th>CHASxAGE</th>\n",
       "      <th>CHASxDIS</th>\n",
       "      <th>CHASxRAD</th>\n",
       "      <th>CHASxTAX</th>\n",
       "      <th>CHASxPTRATIO</th>\n",
       "      <th>CHASxB</th>\n",
       "      <th>CHASxLSTAT</th>\n",
       "      <th>NOXxCRIM</th>\n",
       "      <th>NOXxZN</th>\n",
       "      <th>NOXxINDUS</th>\n",
       "      <th>NOXxCHAS</th>\n",
       "      <th>NOXxNOX</th>\n",
       "      <th>NOXxRM</th>\n",
       "      <th>NOXxAGE</th>\n",
       "      <th>NOXxDIS</th>\n",
       "      <th>NOXxRAD</th>\n",
       "      <th>NOXxTAX</th>\n",
       "      <th>NOXxPTRATIO</th>\n",
       "      <th>NOXxB</th>\n",
       "      <th>NOXxLSTAT</th>\n",
       "      <th>RMxCRIM</th>\n",
       "      <th>RMxZN</th>\n",
       "      <th>RMxINDUS</th>\n",
       "      <th>RMxCHAS</th>\n",
       "      <th>RMxNOX</th>\n",
       "      <th>RMxRM</th>\n",
       "      <th>RMxAGE</th>\n",
       "      <th>RMxDIS</th>\n",
       "      <th>RMxRAD</th>\n",
       "      <th>RMxTAX</th>\n",
       "      <th>RMxPTRATIO</th>\n",
       "      <th>RMxB</th>\n",
       "      <th>RMxLSTAT</th>\n",
       "      <th>AGExCRIM</th>\n",
       "      <th>AGExZN</th>\n",
       "      <th>AGExINDUS</th>\n",
       "      <th>AGExCHAS</th>\n",
       "      <th>AGExNOX</th>\n",
       "      <th>AGExRM</th>\n",
       "      <th>AGExAGE</th>\n",
       "      <th>AGExDIS</th>\n",
       "      <th>AGExRAD</th>\n",
       "      <th>AGExTAX</th>\n",
       "      <th>AGExPTRATIO</th>\n",
       "      <th>AGExB</th>\n",
       "      <th>AGExLSTAT</th>\n",
       "      <th>DISxCRIM</th>\n",
       "      <th>DISxZN</th>\n",
       "      <th>DISxINDUS</th>\n",
       "      <th>DISxCHAS</th>\n",
       "      <th>DISxNOX</th>\n",
       "      <th>DISxRM</th>\n",
       "      <th>DISxAGE</th>\n",
       "      <th>DISxDIS</th>\n",
       "      <th>DISxRAD</th>\n",
       "      <th>DISxTAX</th>\n",
       "      <th>DISxPTRATIO</th>\n",
       "      <th>DISxB</th>\n",
       "      <th>DISxLSTAT</th>\n",
       "      <th>RADxCRIM</th>\n",
       "      <th>RADxZN</th>\n",
       "      <th>RADxINDUS</th>\n",
       "      <th>RADxCHAS</th>\n",
       "      <th>RADxNOX</th>\n",
       "      <th>RADxRM</th>\n",
       "      <th>RADxAGE</th>\n",
       "      <th>RADxDIS</th>\n",
       "      <th>RADxRAD</th>\n",
       "      <th>RADxTAX</th>\n",
       "      <th>RADxPTRATIO</th>\n",
       "      <th>RADxB</th>\n",
       "      <th>RADxLSTAT</th>\n",
       "      <th>TAXxCRIM</th>\n",
       "      <th>TAXxZN</th>\n",
       "      <th>TAXxINDUS</th>\n",
       "      <th>TAXxCHAS</th>\n",
       "      <th>TAXxNOX</th>\n",
       "      <th>TAXxRM</th>\n",
       "      <th>TAXxAGE</th>\n",
       "      <th>TAXxDIS</th>\n",
       "      <th>TAXxRAD</th>\n",
       "      <th>TAXxTAX</th>\n",
       "      <th>TAXxPTRATIO</th>\n",
       "      <th>TAXxB</th>\n",
       "      <th>TAXxLSTAT</th>\n",
       "      <th>PTRATIOxCRIM</th>\n",
       "      <th>PTRATIOxZN</th>\n",
       "      <th>PTRATIOxINDUS</th>\n",
       "      <th>PTRATIOxCHAS</th>\n",
       "      <th>PTRATIOxNOX</th>\n",
       "      <th>PTRATIOxRM</th>\n",
       "      <th>PTRATIOxAGE</th>\n",
       "      <th>PTRATIOxDIS</th>\n",
       "      <th>PTRATIOxRAD</th>\n",
       "      <th>PTRATIOxTAX</th>\n",
       "      <th>PTRATIOxPTRATIO</th>\n",
       "      <th>PTRATIOxB</th>\n",
       "      <th>PTRATIOxLSTAT</th>\n",
       "      <th>BxCRIM</th>\n",
       "      <th>BxZN</th>\n",
       "      <th>BxINDUS</th>\n",
       "      <th>BxCHAS</th>\n",
       "      <th>BxNOX</th>\n",
       "      <th>BxRM</th>\n",
       "      <th>BxAGE</th>\n",
       "      <th>BxDIS</th>\n",
       "      <th>BxRAD</th>\n",
       "      <th>BxTAX</th>\n",
       "      <th>BxPTRATIO</th>\n",
       "      <th>BxB</th>\n",
       "      <th>BxLSTAT</th>\n",
       "      <th>LSTATxCRIM</th>\n",
       "      <th>LSTATxZN</th>\n",
       "      <th>LSTATxINDUS</th>\n",
       "      <th>LSTATxCHAS</th>\n",
       "      <th>LSTATxNOX</th>\n",
       "      <th>LSTATxRM</th>\n",
       "      <th>LSTATxAGE</th>\n",
       "      <th>LSTATxDIS</th>\n",
       "      <th>LSTATxRAD</th>\n",
       "      <th>LSTATxTAX</th>\n",
       "      <th>LSTATxPTRATIO</th>\n",
       "      <th>LSTATxB</th>\n",
       "      <th>LSTATxLSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507</td>\n",
       "      <td>6.951</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>391.70</td>\n",
       "      <td>9.71</td>\n",
       "      <td>0.128228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.220158</td>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.181552</td>\n",
       "      <td>2.489084</td>\n",
       "      <td>31.690965</td>\n",
       "      <td>1.024746</td>\n",
       "      <td>2.86472</td>\n",
       "      <td>109.93363</td>\n",
       "      <td>6.230766</td>\n",
       "      <td>140.263853</td>\n",
       "      <td>3.477054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.220158</td>\n",
       "      <td>0.000</td>\n",
       "      <td>38.4400</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.14340</td>\n",
       "      <td>43.09620</td>\n",
       "      <td>548.700</td>\n",
       "      <td>17.742540</td>\n",
       "      <td>49.60</td>\n",
       "      <td>1903.40</td>\n",
       "      <td>107.880</td>\n",
       "      <td>2428.5400</td>\n",
       "      <td>60.2020</td>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507</td>\n",
       "      <td>6.951</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>391.7</td>\n",
       "      <td>9.71</td>\n",
       "      <td>0.181552</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.14340</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.257049</td>\n",
       "      <td>3.524157</td>\n",
       "      <td>44.8695</td>\n",
       "      <td>1.450882</td>\n",
       "      <td>4.056</td>\n",
       "      <td>155.649</td>\n",
       "      <td>8.8218</td>\n",
       "      <td>198.59190</td>\n",
       "      <td>4.92297</td>\n",
       "      <td>2.489084</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.09620</td>\n",
       "      <td>6.951</td>\n",
       "      <td>3.524157</td>\n",
       "      <td>48.316401</td>\n",
       "      <td>615.1635</td>\n",
       "      <td>19.891677</td>\n",
       "      <td>55.608</td>\n",
       "      <td>2133.957</td>\n",
       "      <td>120.9474</td>\n",
       "      <td>2722.70670</td>\n",
       "      <td>67.49421</td>\n",
       "      <td>31.690965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>548.700</td>\n",
       "      <td>88.5</td>\n",
       "      <td>44.8695</td>\n",
       "      <td>615.1635</td>\n",
       "      <td>7832.25</td>\n",
       "      <td>253.26045</td>\n",
       "      <td>708.0</td>\n",
       "      <td>27169.5</td>\n",
       "      <td>1539.90</td>\n",
       "      <td>34665.450</td>\n",
       "      <td>859.335</td>\n",
       "      <td>1.024746</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>17.742540</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>1.450882</td>\n",
       "      <td>19.891677</td>\n",
       "      <td>253.26045</td>\n",
       "      <td>8.189327</td>\n",
       "      <td>22.8936</td>\n",
       "      <td>878.5419</td>\n",
       "      <td>49.79358</td>\n",
       "      <td>1120.927890</td>\n",
       "      <td>27.787107</td>\n",
       "      <td>2.86472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.60</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.056</td>\n",
       "      <td>55.608</td>\n",
       "      <td>708.0</td>\n",
       "      <td>22.8936</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2456.0</td>\n",
       "      <td>139.2</td>\n",
       "      <td>3133.60</td>\n",
       "      <td>77.68</td>\n",
       "      <td>109.93363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1903.40</td>\n",
       "      <td>307.0</td>\n",
       "      <td>155.649</td>\n",
       "      <td>2133.957</td>\n",
       "      <td>27169.5</td>\n",
       "      <td>878.5419</td>\n",
       "      <td>2456.0</td>\n",
       "      <td>94249.0</td>\n",
       "      <td>5341.8</td>\n",
       "      <td>120251.90</td>\n",
       "      <td>2980.97</td>\n",
       "      <td>6.230766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.880</td>\n",
       "      <td>17.4</td>\n",
       "      <td>8.8218</td>\n",
       "      <td>120.9474</td>\n",
       "      <td>1539.90</td>\n",
       "      <td>49.79358</td>\n",
       "      <td>139.2</td>\n",
       "      <td>5341.8</td>\n",
       "      <td>302.76</td>\n",
       "      <td>6815.580</td>\n",
       "      <td>168.954</td>\n",
       "      <td>140.263853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2428.5400</td>\n",
       "      <td>391.7</td>\n",
       "      <td>198.59190</td>\n",
       "      <td>2722.70670</td>\n",
       "      <td>34665.450</td>\n",
       "      <td>1120.927890</td>\n",
       "      <td>3133.60</td>\n",
       "      <td>120251.90</td>\n",
       "      <td>6815.580</td>\n",
       "      <td>153428.8900</td>\n",
       "      <td>3803.4070</td>\n",
       "      <td>3.477054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>60.2020</td>\n",
       "      <td>9.71</td>\n",
       "      <td>4.92297</td>\n",
       "      <td>67.49421</td>\n",
       "      <td>859.335</td>\n",
       "      <td>27.787107</td>\n",
       "      <td>77.68</td>\n",
       "      <td>2980.97</td>\n",
       "      <td>168.954</td>\n",
       "      <td>3803.4070</td>\n",
       "      <td>94.2841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.15876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413</td>\n",
       "      <td>5.961</td>\n",
       "      <td>17.5</td>\n",
       "      <td>5.2873</td>\n",
       "      <td>4.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>376.94</td>\n",
       "      <td>9.88</td>\n",
       "      <td>0.025205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.716196</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.065568</td>\n",
       "      <td>0.946368</td>\n",
       "      <td>2.778300</td>\n",
       "      <td>0.839412</td>\n",
       "      <td>0.63504</td>\n",
       "      <td>48.42180</td>\n",
       "      <td>3.048192</td>\n",
       "      <td>59.842994</td>\n",
       "      <td>1.568549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.716196</td>\n",
       "      <td>0.000</td>\n",
       "      <td>116.8561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.46453</td>\n",
       "      <td>64.43841</td>\n",
       "      <td>189.175</td>\n",
       "      <td>57.155713</td>\n",
       "      <td>43.24</td>\n",
       "      <td>3297.05</td>\n",
       "      <td>207.552</td>\n",
       "      <td>4074.7214</td>\n",
       "      <td>106.8028</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.065568</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.46453</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.170569</td>\n",
       "      <td>2.461893</td>\n",
       "      <td>7.2275</td>\n",
       "      <td>2.183655</td>\n",
       "      <td>1.652</td>\n",
       "      <td>125.965</td>\n",
       "      <td>7.9296</td>\n",
       "      <td>155.67622</td>\n",
       "      <td>4.08044</td>\n",
       "      <td>0.946368</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64.43841</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.461893</td>\n",
       "      <td>35.533521</td>\n",
       "      <td>104.3175</td>\n",
       "      <td>31.517595</td>\n",
       "      <td>23.844</td>\n",
       "      <td>1818.105</td>\n",
       "      <td>114.4512</td>\n",
       "      <td>2246.93934</td>\n",
       "      <td>58.89468</td>\n",
       "      <td>2.778300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>189.175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2275</td>\n",
       "      <td>104.3175</td>\n",
       "      <td>306.25</td>\n",
       "      <td>92.52775</td>\n",
       "      <td>70.0</td>\n",
       "      <td>5337.5</td>\n",
       "      <td>336.00</td>\n",
       "      <td>6596.450</td>\n",
       "      <td>172.900</td>\n",
       "      <td>0.839412</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>57.155713</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.183655</td>\n",
       "      <td>31.517595</td>\n",
       "      <td>92.52775</td>\n",
       "      <td>27.955541</td>\n",
       "      <td>21.1492</td>\n",
       "      <td>1612.6265</td>\n",
       "      <td>101.51616</td>\n",
       "      <td>1992.994862</td>\n",
       "      <td>52.238524</td>\n",
       "      <td>0.63504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.652</td>\n",
       "      <td>23.844</td>\n",
       "      <td>70.0</td>\n",
       "      <td>21.1492</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>76.8</td>\n",
       "      <td>1507.76</td>\n",
       "      <td>39.52</td>\n",
       "      <td>48.42180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3297.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125.965</td>\n",
       "      <td>1818.105</td>\n",
       "      <td>5337.5</td>\n",
       "      <td>1612.6265</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>93025.0</td>\n",
       "      <td>5856.0</td>\n",
       "      <td>114966.70</td>\n",
       "      <td>3013.40</td>\n",
       "      <td>3.048192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>207.552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9296</td>\n",
       "      <td>114.4512</td>\n",
       "      <td>336.00</td>\n",
       "      <td>101.51616</td>\n",
       "      <td>76.8</td>\n",
       "      <td>5856.0</td>\n",
       "      <td>368.64</td>\n",
       "      <td>7237.248</td>\n",
       "      <td>189.696</td>\n",
       "      <td>59.842994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4074.7214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155.67622</td>\n",
       "      <td>2246.93934</td>\n",
       "      <td>6596.450</td>\n",
       "      <td>1992.994862</td>\n",
       "      <td>1507.76</td>\n",
       "      <td>114966.70</td>\n",
       "      <td>7237.248</td>\n",
       "      <td>142083.7636</td>\n",
       "      <td>3724.1672</td>\n",
       "      <td>1.568549</td>\n",
       "      <td>0.000</td>\n",
       "      <td>106.8028</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.08044</td>\n",
       "      <td>58.89468</td>\n",
       "      <td>172.900</td>\n",
       "      <td>52.238524</td>\n",
       "      <td>39.52</td>\n",
       "      <td>3013.40</td>\n",
       "      <td>189.696</td>\n",
       "      <td>3724.1672</td>\n",
       "      <td>97.6144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.11329</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428</td>\n",
       "      <td>6.897</td>\n",
       "      <td>54.3</td>\n",
       "      <td>6.3361</td>\n",
       "      <td>6.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>391.25</td>\n",
       "      <td>11.38</td>\n",
       "      <td>0.012835</td>\n",
       "      <td>3.398700</td>\n",
       "      <td>0.558520</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.048488</td>\n",
       "      <td>0.781361</td>\n",
       "      <td>6.151647</td>\n",
       "      <td>0.717817</td>\n",
       "      <td>0.67974</td>\n",
       "      <td>33.98700</td>\n",
       "      <td>1.880614</td>\n",
       "      <td>44.324713</td>\n",
       "      <td>1.289240</td>\n",
       "      <td>3.398700</td>\n",
       "      <td>900.00</td>\n",
       "      <td>147.900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.84</td>\n",
       "      <td>206.91</td>\n",
       "      <td>1629.0</td>\n",
       "      <td>190.08300</td>\n",
       "      <td>180.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>11737.5</td>\n",
       "      <td>341.400</td>\n",
       "      <td>0.558520</td>\n",
       "      <td>147.900</td>\n",
       "      <td>24.3049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.11004</td>\n",
       "      <td>34.00221</td>\n",
       "      <td>267.699</td>\n",
       "      <td>31.236973</td>\n",
       "      <td>29.58</td>\n",
       "      <td>1479.00</td>\n",
       "      <td>81.838</td>\n",
       "      <td>1928.8625</td>\n",
       "      <td>56.1034</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.048488</td>\n",
       "      <td>12.84</td>\n",
       "      <td>2.11004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.183184</td>\n",
       "      <td>2.951916</td>\n",
       "      <td>23.2404</td>\n",
       "      <td>2.711851</td>\n",
       "      <td>2.568</td>\n",
       "      <td>128.400</td>\n",
       "      <td>7.1048</td>\n",
       "      <td>167.45500</td>\n",
       "      <td>4.87064</td>\n",
       "      <td>0.781361</td>\n",
       "      <td>206.91</td>\n",
       "      <td>34.00221</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.951916</td>\n",
       "      <td>47.568609</td>\n",
       "      <td>374.5071</td>\n",
       "      <td>43.700082</td>\n",
       "      <td>41.382</td>\n",
       "      <td>2069.100</td>\n",
       "      <td>114.4902</td>\n",
       "      <td>2698.45125</td>\n",
       "      <td>78.48786</td>\n",
       "      <td>6.151647</td>\n",
       "      <td>1629.0</td>\n",
       "      <td>267.699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.2404</td>\n",
       "      <td>374.5071</td>\n",
       "      <td>2948.49</td>\n",
       "      <td>344.05023</td>\n",
       "      <td>325.8</td>\n",
       "      <td>16290.0</td>\n",
       "      <td>901.38</td>\n",
       "      <td>21244.875</td>\n",
       "      <td>617.934</td>\n",
       "      <td>0.717817</td>\n",
       "      <td>190.08300</td>\n",
       "      <td>31.236973</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.711851</td>\n",
       "      <td>43.700082</td>\n",
       "      <td>344.05023</td>\n",
       "      <td>40.146163</td>\n",
       "      <td>38.0166</td>\n",
       "      <td>1900.8300</td>\n",
       "      <td>105.17926</td>\n",
       "      <td>2478.999125</td>\n",
       "      <td>72.104818</td>\n",
       "      <td>0.67974</td>\n",
       "      <td>180.0</td>\n",
       "      <td>29.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.568</td>\n",
       "      <td>41.382</td>\n",
       "      <td>325.8</td>\n",
       "      <td>38.0166</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>99.6</td>\n",
       "      <td>2347.50</td>\n",
       "      <td>68.28</td>\n",
       "      <td>33.98700</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>1479.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128.400</td>\n",
       "      <td>2069.100</td>\n",
       "      <td>16290.0</td>\n",
       "      <td>1900.8300</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>4980.0</td>\n",
       "      <td>117375.00</td>\n",
       "      <td>3414.00</td>\n",
       "      <td>1.880614</td>\n",
       "      <td>498.0</td>\n",
       "      <td>81.838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.1048</td>\n",
       "      <td>114.4902</td>\n",
       "      <td>901.38</td>\n",
       "      <td>105.17926</td>\n",
       "      <td>99.6</td>\n",
       "      <td>4980.0</td>\n",
       "      <td>275.56</td>\n",
       "      <td>6494.750</td>\n",
       "      <td>188.908</td>\n",
       "      <td>44.324713</td>\n",
       "      <td>11737.5</td>\n",
       "      <td>1928.8625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>167.45500</td>\n",
       "      <td>2698.45125</td>\n",
       "      <td>21244.875</td>\n",
       "      <td>2478.999125</td>\n",
       "      <td>2347.50</td>\n",
       "      <td>117375.00</td>\n",
       "      <td>6494.750</td>\n",
       "      <td>153076.5625</td>\n",
       "      <td>4452.4250</td>\n",
       "      <td>1.289240</td>\n",
       "      <td>341.400</td>\n",
       "      <td>56.1034</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.87064</td>\n",
       "      <td>78.48786</td>\n",
       "      <td>617.934</td>\n",
       "      <td>72.104818</td>\n",
       "      <td>68.28</td>\n",
       "      <td>3414.00</td>\n",
       "      <td>188.908</td>\n",
       "      <td>4452.4250</td>\n",
       "      <td>129.5044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "      <td>0.007795</td>\n",
       "      <td>1.103625</td>\n",
       "      <td>0.694842</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.046264</td>\n",
       "      <td>0.530799</td>\n",
       "      <td>5.880114</td>\n",
       "      <td>0.490937</td>\n",
       "      <td>0.44145</td>\n",
       "      <td>27.45819</td>\n",
       "      <td>1.342008</td>\n",
       "      <td>34.927524</td>\n",
       "      <td>1.097445</td>\n",
       "      <td>1.103625</td>\n",
       "      <td>156.25</td>\n",
       "      <td>98.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.55</td>\n",
       "      <td>75.15</td>\n",
       "      <td>832.5</td>\n",
       "      <td>69.50625</td>\n",
       "      <td>62.5</td>\n",
       "      <td>3887.5</td>\n",
       "      <td>190.0</td>\n",
       "      <td>4945.0</td>\n",
       "      <td>155.375</td>\n",
       "      <td>0.694842</td>\n",
       "      <td>98.375</td>\n",
       "      <td>61.9369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.12388</td>\n",
       "      <td>47.31444</td>\n",
       "      <td>524.142</td>\n",
       "      <td>43.761135</td>\n",
       "      <td>39.35</td>\n",
       "      <td>2447.57</td>\n",
       "      <td>119.624</td>\n",
       "      <td>3113.3720</td>\n",
       "      <td>97.8241</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.046264</td>\n",
       "      <td>6.55</td>\n",
       "      <td>4.12388</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.274576</td>\n",
       "      <td>3.150288</td>\n",
       "      <td>34.8984</td>\n",
       "      <td>2.913702</td>\n",
       "      <td>2.620</td>\n",
       "      <td>162.964</td>\n",
       "      <td>7.9648</td>\n",
       "      <td>207.29440</td>\n",
       "      <td>6.51332</td>\n",
       "      <td>0.530799</td>\n",
       "      <td>75.15</td>\n",
       "      <td>47.31444</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.150288</td>\n",
       "      <td>36.144144</td>\n",
       "      <td>400.3992</td>\n",
       "      <td>33.429726</td>\n",
       "      <td>30.060</td>\n",
       "      <td>1869.732</td>\n",
       "      <td>91.3824</td>\n",
       "      <td>2378.34720</td>\n",
       "      <td>74.72916</td>\n",
       "      <td>5.880114</td>\n",
       "      <td>832.5</td>\n",
       "      <td>524.142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.8984</td>\n",
       "      <td>400.3992</td>\n",
       "      <td>4435.56</td>\n",
       "      <td>370.32930</td>\n",
       "      <td>333.0</td>\n",
       "      <td>20712.6</td>\n",
       "      <td>1012.32</td>\n",
       "      <td>26346.960</td>\n",
       "      <td>827.838</td>\n",
       "      <td>0.490937</td>\n",
       "      <td>69.50625</td>\n",
       "      <td>43.761135</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.913702</td>\n",
       "      <td>33.429726</td>\n",
       "      <td>370.32930</td>\n",
       "      <td>30.919160</td>\n",
       "      <td>27.8025</td>\n",
       "      <td>1729.3155</td>\n",
       "      <td>84.51960</td>\n",
       "      <td>2199.733800</td>\n",
       "      <td>69.117015</td>\n",
       "      <td>0.44145</td>\n",
       "      <td>62.5</td>\n",
       "      <td>39.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.620</td>\n",
       "      <td>30.060</td>\n",
       "      <td>333.0</td>\n",
       "      <td>27.8025</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1555.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1978.00</td>\n",
       "      <td>62.15</td>\n",
       "      <td>27.45819</td>\n",
       "      <td>3887.5</td>\n",
       "      <td>2447.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.964</td>\n",
       "      <td>1869.732</td>\n",
       "      <td>20712.6</td>\n",
       "      <td>1729.3155</td>\n",
       "      <td>1555.0</td>\n",
       "      <td>96721.0</td>\n",
       "      <td>4727.2</td>\n",
       "      <td>123031.60</td>\n",
       "      <td>3865.73</td>\n",
       "      <td>1.342008</td>\n",
       "      <td>190.0</td>\n",
       "      <td>119.624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9648</td>\n",
       "      <td>91.3824</td>\n",
       "      <td>1012.32</td>\n",
       "      <td>84.51960</td>\n",
       "      <td>76.0</td>\n",
       "      <td>4727.2</td>\n",
       "      <td>231.04</td>\n",
       "      <td>6013.120</td>\n",
       "      <td>188.936</td>\n",
       "      <td>34.927524</td>\n",
       "      <td>4945.0</td>\n",
       "      <td>3113.3720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>207.29440</td>\n",
       "      <td>2378.34720</td>\n",
       "      <td>26346.960</td>\n",
       "      <td>2199.733800</td>\n",
       "      <td>1978.00</td>\n",
       "      <td>123031.60</td>\n",
       "      <td>6013.120</td>\n",
       "      <td>156499.3600</td>\n",
       "      <td>4917.3080</td>\n",
       "      <td>1.097445</td>\n",
       "      <td>155.375</td>\n",
       "      <td>97.8241</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.51332</td>\n",
       "      <td>74.72916</td>\n",
       "      <td>827.838</td>\n",
       "      <td>69.117015</td>\n",
       "      <td>62.15</td>\n",
       "      <td>3865.73</td>\n",
       "      <td>188.936</td>\n",
       "      <td>4917.3080</td>\n",
       "      <td>154.5049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>25.94060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.679</td>\n",
       "      <td>5.304</td>\n",
       "      <td>89.1</td>\n",
       "      <td>1.6475</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>127.36</td>\n",
       "      <td>26.64</td>\n",
       "      <td>672.914728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>469.524860</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>17.613667</td>\n",
       "      <td>137.588942</td>\n",
       "      <td>2311.307460</td>\n",
       "      <td>42.737139</td>\n",
       "      <td>622.57440</td>\n",
       "      <td>17276.43960</td>\n",
       "      <td>524.000120</td>\n",
       "      <td>3303.794816</td>\n",
       "      <td>691.057584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>469.524860</td>\n",
       "      <td>0.000</td>\n",
       "      <td>327.6100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.28990</td>\n",
       "      <td>96.00240</td>\n",
       "      <td>1612.710</td>\n",
       "      <td>29.819750</td>\n",
       "      <td>434.40</td>\n",
       "      <td>12054.60</td>\n",
       "      <td>365.620</td>\n",
       "      <td>2305.2160</td>\n",
       "      <td>482.1840</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.613667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.28990</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.461041</td>\n",
       "      <td>3.601416</td>\n",
       "      <td>60.4989</td>\n",
       "      <td>1.118653</td>\n",
       "      <td>16.296</td>\n",
       "      <td>452.214</td>\n",
       "      <td>13.7158</td>\n",
       "      <td>86.47744</td>\n",
       "      <td>18.08856</td>\n",
       "      <td>137.588942</td>\n",
       "      <td>0.00</td>\n",
       "      <td>96.00240</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.601416</td>\n",
       "      <td>28.132416</td>\n",
       "      <td>472.5864</td>\n",
       "      <td>8.738340</td>\n",
       "      <td>127.296</td>\n",
       "      <td>3532.464</td>\n",
       "      <td>107.1408</td>\n",
       "      <td>675.51744</td>\n",
       "      <td>141.29856</td>\n",
       "      <td>2311.307460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1612.710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.4989</td>\n",
       "      <td>472.5864</td>\n",
       "      <td>7938.81</td>\n",
       "      <td>146.79225</td>\n",
       "      <td>2138.4</td>\n",
       "      <td>59340.6</td>\n",
       "      <td>1799.82</td>\n",
       "      <td>11347.776</td>\n",
       "      <td>2373.624</td>\n",
       "      <td>42.737139</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>29.819750</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.118653</td>\n",
       "      <td>8.738340</td>\n",
       "      <td>146.79225</td>\n",
       "      <td>2.714256</td>\n",
       "      <td>39.5400</td>\n",
       "      <td>1097.2350</td>\n",
       "      <td>33.27950</td>\n",
       "      <td>209.825600</td>\n",
       "      <td>43.889400</td>\n",
       "      <td>622.57440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>434.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.296</td>\n",
       "      <td>127.296</td>\n",
       "      <td>2138.4</td>\n",
       "      <td>39.5400</td>\n",
       "      <td>576.0</td>\n",
       "      <td>15984.0</td>\n",
       "      <td>484.8</td>\n",
       "      <td>3056.64</td>\n",
       "      <td>639.36</td>\n",
       "      <td>17276.43960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12054.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>452.214</td>\n",
       "      <td>3532.464</td>\n",
       "      <td>59340.6</td>\n",
       "      <td>1097.2350</td>\n",
       "      <td>15984.0</td>\n",
       "      <td>443556.0</td>\n",
       "      <td>13453.2</td>\n",
       "      <td>84821.76</td>\n",
       "      <td>17742.24</td>\n",
       "      <td>524.000120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365.620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.7158</td>\n",
       "      <td>107.1408</td>\n",
       "      <td>1799.82</td>\n",
       "      <td>33.27950</td>\n",
       "      <td>484.8</td>\n",
       "      <td>13453.2</td>\n",
       "      <td>408.04</td>\n",
       "      <td>2572.672</td>\n",
       "      <td>538.128</td>\n",
       "      <td>3303.794816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2305.2160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.47744</td>\n",
       "      <td>675.51744</td>\n",
       "      <td>11347.776</td>\n",
       "      <td>209.825600</td>\n",
       "      <td>3056.64</td>\n",
       "      <td>84821.76</td>\n",
       "      <td>2572.672</td>\n",
       "      <td>16220.5696</td>\n",
       "      <td>3392.8704</td>\n",
       "      <td>691.057584</td>\n",
       "      <td>0.000</td>\n",
       "      <td>482.1840</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.08856</td>\n",
       "      <td>141.29856</td>\n",
       "      <td>2373.624</td>\n",
       "      <td>43.889400</td>\n",
       "      <td>639.36</td>\n",
       "      <td>17742.24</td>\n",
       "      <td>538.128</td>\n",
       "      <td>3392.8704</td>\n",
       "      <td>709.6896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS   RAD    TAX  \\\n",
       "220   0.35809   0.0   6.20   1.0  0.507  6.951  88.5  2.8617   8.0  307.0   \n",
       "71    0.15876   0.0  10.81   0.0  0.413  5.961  17.5  5.2873   4.0  305.0   \n",
       "240   0.11329  30.0   4.93   0.0  0.428  6.897  54.3  6.3361   6.0  300.0   \n",
       "6     0.08829  12.5   7.87   0.0  0.524  6.012  66.6  5.5605   5.0  311.0   \n",
       "417  25.94060   0.0  18.10   0.0  0.679  5.304  89.1  1.6475  24.0  666.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT   CRIMxCRIM   CRIMxZN  CRIMxINDUS  CRIMxCHAS  \\\n",
       "220     17.4  391.70   9.71    0.128228  0.000000    2.220158    0.35809   \n",
       "71      19.2  376.94   9.88    0.025205  0.000000    1.716196    0.00000   \n",
       "240     16.6  391.25  11.38    0.012835  3.398700    0.558520    0.00000   \n",
       "6       15.2  395.60  12.43    0.007795  1.103625    0.694842    0.00000   \n",
       "417     20.2  127.36  26.64  672.914728  0.000000  469.524860    0.00000   \n",
       "\n",
       "      CRIMxNOX     CRIMxRM     CRIMxAGE   CRIMxDIS   CRIMxRAD     CRIMxTAX  \\\n",
       "220   0.181552    2.489084    31.690965   1.024746    2.86472    109.93363   \n",
       "71    0.065568    0.946368     2.778300   0.839412    0.63504     48.42180   \n",
       "240   0.048488    0.781361     6.151647   0.717817    0.67974     33.98700   \n",
       "6     0.046264    0.530799     5.880114   0.490937    0.44145     27.45819   \n",
       "417  17.613667  137.588942  2311.307460  42.737139  622.57440  17276.43960   \n",
       "\n",
       "     CRIMxPTRATIO       CRIMxB  CRIMxLSTAT   ZNxCRIM   ZNxZN  ZNxINDUS  \\\n",
       "220      6.230766   140.263853    3.477054  0.000000    0.00     0.000   \n",
       "71       3.048192    59.842994    1.568549  0.000000    0.00     0.000   \n",
       "240      1.880614    44.324713    1.289240  3.398700  900.00   147.900   \n",
       "6        1.342008    34.927524    1.097445  1.103625  156.25    98.375   \n",
       "417    524.000120  3303.794816  691.057584  0.000000    0.00     0.000   \n",
       "\n",
       "     ZNxCHAS  ZNxNOX   ZNxRM  ZNxAGE     ZNxDIS  ZNxRAD  ZNxTAX  ZNxPTRATIO  \\\n",
       "220      0.0    0.00    0.00     0.0    0.00000     0.0     0.0         0.0   \n",
       "71       0.0    0.00    0.00     0.0    0.00000     0.0     0.0         0.0   \n",
       "240      0.0   12.84  206.91  1629.0  190.08300   180.0  9000.0       498.0   \n",
       "6        0.0    6.55   75.15   832.5   69.50625    62.5  3887.5       190.0   \n",
       "417      0.0    0.00    0.00     0.0    0.00000     0.0     0.0         0.0   \n",
       "\n",
       "        ZNxB  ZNxLSTAT  INDUSxCRIM  INDUSxZN  INDUSxINDUS  INDUSxCHAS  \\\n",
       "220      0.0     0.000    2.220158     0.000      38.4400         6.2   \n",
       "71       0.0     0.000    1.716196     0.000     116.8561         0.0   \n",
       "240  11737.5   341.400    0.558520   147.900      24.3049         0.0   \n",
       "6     4945.0   155.375    0.694842    98.375      61.9369         0.0   \n",
       "417      0.0     0.000  469.524860     0.000     327.6100         0.0   \n",
       "\n",
       "     INDUSxNOX  INDUSxRM  INDUSxAGE  INDUSxDIS  INDUSxRAD  INDUSxTAX  \\\n",
       "220    3.14340  43.09620    548.700  17.742540      49.60    1903.40   \n",
       "71     4.46453  64.43841    189.175  57.155713      43.24    3297.05   \n",
       "240    2.11004  34.00221    267.699  31.236973      29.58    1479.00   \n",
       "6      4.12388  47.31444    524.142  43.761135      39.35    2447.57   \n",
       "417   12.28990  96.00240   1612.710  29.819750     434.40   12054.60   \n",
       "\n",
       "     INDUSxPTRATIO    INDUSxB  INDUSxLSTAT  CHASxCRIM  CHASxZN  CHASxINDUS  \\\n",
       "220        107.880  2428.5400      60.2020    0.35809      0.0         6.2   \n",
       "71         207.552  4074.7214     106.8028    0.00000      0.0         0.0   \n",
       "240         81.838  1928.8625      56.1034    0.00000      0.0         0.0   \n",
       "6          119.624  3113.3720      97.8241    0.00000      0.0         0.0   \n",
       "417        365.620  2305.2160     482.1840    0.00000      0.0         0.0   \n",
       "\n",
       "     CHASxCHAS  CHASxNOX  CHASxRM  CHASxAGE  CHASxDIS  CHASxRAD  CHASxTAX  \\\n",
       "220        1.0     0.507    6.951      88.5    2.8617       8.0     307.0   \n",
       "71         0.0     0.000    0.000       0.0    0.0000       0.0       0.0   \n",
       "240        0.0     0.000    0.000       0.0    0.0000       0.0       0.0   \n",
       "6          0.0     0.000    0.000       0.0    0.0000       0.0       0.0   \n",
       "417        0.0     0.000    0.000       0.0    0.0000       0.0       0.0   \n",
       "\n",
       "     CHASxPTRATIO  CHASxB  CHASxLSTAT   NOXxCRIM  NOXxZN  NOXxINDUS  NOXxCHAS  \\\n",
       "220          17.4   391.7        9.71   0.181552    0.00    3.14340     0.507   \n",
       "71            0.0     0.0        0.00   0.065568    0.00    4.46453     0.000   \n",
       "240           0.0     0.0        0.00   0.048488   12.84    2.11004     0.000   \n",
       "6             0.0     0.0        0.00   0.046264    6.55    4.12388     0.000   \n",
       "417           0.0     0.0        0.00  17.613667    0.00   12.28990     0.000   \n",
       "\n",
       "      NOXxNOX    NOXxRM  NOXxAGE   NOXxDIS  NOXxRAD  NOXxTAX  NOXxPTRATIO  \\\n",
       "220  0.257049  3.524157  44.8695  1.450882    4.056  155.649       8.8218   \n",
       "71   0.170569  2.461893   7.2275  2.183655    1.652  125.965       7.9296   \n",
       "240  0.183184  2.951916  23.2404  2.711851    2.568  128.400       7.1048   \n",
       "6    0.274576  3.150288  34.8984  2.913702    2.620  162.964       7.9648   \n",
       "417  0.461041  3.601416  60.4989  1.118653   16.296  452.214      13.7158   \n",
       "\n",
       "         NOXxB  NOXxLSTAT     RMxCRIM   RMxZN  RMxINDUS  RMxCHAS    RMxNOX  \\\n",
       "220  198.59190    4.92297    2.489084    0.00  43.09620    6.951  3.524157   \n",
       "71   155.67622    4.08044    0.946368    0.00  64.43841    0.000  2.461893   \n",
       "240  167.45500    4.87064    0.781361  206.91  34.00221    0.000  2.951916   \n",
       "6    207.29440    6.51332    0.530799   75.15  47.31444    0.000  3.150288   \n",
       "417   86.47744   18.08856  137.588942    0.00  96.00240    0.000  3.601416   \n",
       "\n",
       "         RMxRM    RMxAGE     RMxDIS   RMxRAD    RMxTAX  RMxPTRATIO  \\\n",
       "220  48.316401  615.1635  19.891677   55.608  2133.957    120.9474   \n",
       "71   35.533521  104.3175  31.517595   23.844  1818.105    114.4512   \n",
       "240  47.568609  374.5071  43.700082   41.382  2069.100    114.4902   \n",
       "6    36.144144  400.3992  33.429726   30.060  1869.732     91.3824   \n",
       "417  28.132416  472.5864   8.738340  127.296  3532.464    107.1408   \n",
       "\n",
       "           RMxB   RMxLSTAT     AGExCRIM  AGExZN  AGExINDUS  AGExCHAS  AGExNOX  \\\n",
       "220  2722.70670   67.49421    31.690965     0.0    548.700      88.5  44.8695   \n",
       "71   2246.93934   58.89468     2.778300     0.0    189.175       0.0   7.2275   \n",
       "240  2698.45125   78.48786     6.151647  1629.0    267.699       0.0  23.2404   \n",
       "6    2378.34720   74.72916     5.880114   832.5    524.142       0.0  34.8984   \n",
       "417   675.51744  141.29856  2311.307460     0.0   1612.710       0.0  60.4989   \n",
       "\n",
       "       AGExRM  AGExAGE    AGExDIS  AGExRAD  AGExTAX  AGExPTRATIO      AGExB  \\\n",
       "220  615.1635  7832.25  253.26045    708.0  27169.5      1539.90  34665.450   \n",
       "71   104.3175   306.25   92.52775     70.0   5337.5       336.00   6596.450   \n",
       "240  374.5071  2948.49  344.05023    325.8  16290.0       901.38  21244.875   \n",
       "6    400.3992  4435.56  370.32930    333.0  20712.6      1012.32  26346.960   \n",
       "417  472.5864  7938.81  146.79225   2138.4  59340.6      1799.82  11347.776   \n",
       "\n",
       "     AGExLSTAT   DISxCRIM     DISxZN  DISxINDUS  DISxCHAS   DISxNOX  \\\n",
       "220    859.335   1.024746    0.00000  17.742540    2.8617  1.450882   \n",
       "71     172.900   0.839412    0.00000  57.155713    0.0000  2.183655   \n",
       "240    617.934   0.717817  190.08300  31.236973    0.0000  2.711851   \n",
       "6      827.838   0.490937   69.50625  43.761135    0.0000  2.913702   \n",
       "417   2373.624  42.737139    0.00000  29.819750    0.0000  1.118653   \n",
       "\n",
       "        DISxRM    DISxAGE    DISxDIS  DISxRAD    DISxTAX  DISxPTRATIO  \\\n",
       "220  19.891677  253.26045   8.189327  22.8936   878.5419     49.79358   \n",
       "71   31.517595   92.52775  27.955541  21.1492  1612.6265    101.51616   \n",
       "240  43.700082  344.05023  40.146163  38.0166  1900.8300    105.17926   \n",
       "6    33.429726  370.32930  30.919160  27.8025  1729.3155     84.51960   \n",
       "417   8.738340  146.79225   2.714256  39.5400  1097.2350     33.27950   \n",
       "\n",
       "           DISxB  DISxLSTAT   RADxCRIM  RADxZN  RADxINDUS  RADxCHAS  RADxNOX  \\\n",
       "220  1120.927890  27.787107    2.86472     0.0      49.60       8.0    4.056   \n",
       "71   1992.994862  52.238524    0.63504     0.0      43.24       0.0    1.652   \n",
       "240  2478.999125  72.104818    0.67974   180.0      29.58       0.0    2.568   \n",
       "6    2199.733800  69.117015    0.44145    62.5      39.35       0.0    2.620   \n",
       "417   209.825600  43.889400  622.57440     0.0     434.40       0.0   16.296   \n",
       "\n",
       "      RADxRM  RADxAGE  RADxDIS  RADxRAD  RADxTAX  RADxPTRATIO    RADxB  \\\n",
       "220   55.608    708.0  22.8936     64.0   2456.0        139.2  3133.60   \n",
       "71    23.844     70.0  21.1492     16.0   1220.0         76.8  1507.76   \n",
       "240   41.382    325.8  38.0166     36.0   1800.0         99.6  2347.50   \n",
       "6     30.060    333.0  27.8025     25.0   1555.0         76.0  1978.00   \n",
       "417  127.296   2138.4  39.5400    576.0  15984.0        484.8  3056.64   \n",
       "\n",
       "     RADxLSTAT     TAXxCRIM  TAXxZN  TAXxINDUS  TAXxCHAS  TAXxNOX    TAXxRM  \\\n",
       "220      77.68    109.93363     0.0    1903.40     307.0  155.649  2133.957   \n",
       "71       39.52     48.42180     0.0    3297.05       0.0  125.965  1818.105   \n",
       "240      68.28     33.98700  9000.0    1479.00       0.0  128.400  2069.100   \n",
       "6        62.15     27.45819  3887.5    2447.57       0.0  162.964  1869.732   \n",
       "417     639.36  17276.43960     0.0   12054.60       0.0  452.214  3532.464   \n",
       "\n",
       "     TAXxAGE    TAXxDIS  TAXxRAD   TAXxTAX  TAXxPTRATIO      TAXxB  TAXxLSTAT  \\\n",
       "220  27169.5   878.5419   2456.0   94249.0       5341.8  120251.90    2980.97   \n",
       "71    5337.5  1612.6265   1220.0   93025.0       5856.0  114966.70    3013.40   \n",
       "240  16290.0  1900.8300   1800.0   90000.0       4980.0  117375.00    3414.00   \n",
       "6    20712.6  1729.3155   1555.0   96721.0       4727.2  123031.60    3865.73   \n",
       "417  59340.6  1097.2350  15984.0  443556.0      13453.2   84821.76   17742.24   \n",
       "\n",
       "     PTRATIOxCRIM  PTRATIOxZN  PTRATIOxINDUS  PTRATIOxCHAS  PTRATIOxNOX  \\\n",
       "220      6.230766         0.0        107.880          17.4       8.8218   \n",
       "71       3.048192         0.0        207.552           0.0       7.9296   \n",
       "240      1.880614       498.0         81.838           0.0       7.1048   \n",
       "6        1.342008       190.0        119.624           0.0       7.9648   \n",
       "417    524.000120         0.0        365.620           0.0      13.7158   \n",
       "\n",
       "     PTRATIOxRM  PTRATIOxAGE  PTRATIOxDIS  PTRATIOxRAD  PTRATIOxTAX  \\\n",
       "220    120.9474      1539.90     49.79358        139.2       5341.8   \n",
       "71     114.4512       336.00    101.51616         76.8       5856.0   \n",
       "240    114.4902       901.38    105.17926         99.6       4980.0   \n",
       "6       91.3824      1012.32     84.51960         76.0       4727.2   \n",
       "417    107.1408      1799.82     33.27950        484.8      13453.2   \n",
       "\n",
       "     PTRATIOxPTRATIO  PTRATIOxB  PTRATIOxLSTAT       BxCRIM     BxZN  \\\n",
       "220           302.76   6815.580        168.954   140.263853      0.0   \n",
       "71            368.64   7237.248        189.696    59.842994      0.0   \n",
       "240           275.56   6494.750        188.908    44.324713  11737.5   \n",
       "6             231.04   6013.120        188.936    34.927524   4945.0   \n",
       "417           408.04   2572.672        538.128  3303.794816      0.0   \n",
       "\n",
       "       BxINDUS  BxCHAS      BxNOX        BxRM      BxAGE        BxDIS  \\\n",
       "220  2428.5400   391.7  198.59190  2722.70670  34665.450  1120.927890   \n",
       "71   4074.7214     0.0  155.67622  2246.93934   6596.450  1992.994862   \n",
       "240  1928.8625     0.0  167.45500  2698.45125  21244.875  2478.999125   \n",
       "6    3113.3720     0.0  207.29440  2378.34720  26346.960  2199.733800   \n",
       "417  2305.2160     0.0   86.47744   675.51744  11347.776   209.825600   \n",
       "\n",
       "       BxRAD      BxTAX  BxPTRATIO          BxB    BxLSTAT  LSTATxCRIM  \\\n",
       "220  3133.60  120251.90   6815.580  153428.8900  3803.4070    3.477054   \n",
       "71   1507.76  114966.70   7237.248  142083.7636  3724.1672    1.568549   \n",
       "240  2347.50  117375.00   6494.750  153076.5625  4452.4250    1.289240   \n",
       "6    1978.00  123031.60   6013.120  156499.3600  4917.3080    1.097445   \n",
       "417  3056.64   84821.76   2572.672   16220.5696  3392.8704  691.057584   \n",
       "\n",
       "     LSTATxZN  LSTATxINDUS  LSTATxCHAS  LSTATxNOX   LSTATxRM  LSTATxAGE  \\\n",
       "220     0.000      60.2020        9.71    4.92297   67.49421    859.335   \n",
       "71      0.000     106.8028        0.00    4.08044   58.89468    172.900   \n",
       "240   341.400      56.1034        0.00    4.87064   78.48786    617.934   \n",
       "6     155.375      97.8241        0.00    6.51332   74.72916    827.838   \n",
       "417     0.000     482.1840        0.00   18.08856  141.29856   2373.624   \n",
       "\n",
       "     LSTATxDIS  LSTATxRAD  LSTATxTAX  LSTATxPTRATIO    LSTATxB  LSTATxLSTAT  \n",
       "220  27.787107      77.68    2980.97        168.954  3803.4070      94.2841  \n",
       "71   52.238524      39.52    3013.40        189.696  3724.1672      97.6144  \n",
       "240  72.104818      68.28    3414.00        188.908  4452.4250     129.5044  \n",
       "6    69.117015      62.15    3865.73        188.936  4917.3080     154.5049  \n",
       "417  43.889400     639.36   17742.24        538.128  3392.8704     709.6896  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13843dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 182)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_x_train.shape #404개의 데이터, 182개의 column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0995e30c",
   "metadata": {},
   "source": [
    "#### 특성확장된 train 데이터의 값과 정답으로 모델을 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f4d44292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2 = LinearRegression()\n",
    "lr2.fit(extended_x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "69c22756",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_x_test = x_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b2826fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6712\\1907752295.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]\n"
     ]
    }
   ],
   "source": [
    "for col1 in x_train.columns : #x_train과 x_test의 columns는 동일하므로 조건문 바꿀 필요 없음\n",
    "    for col2 in x_train.columns : #13번 반복\n",
    "        extended_x_test[col1 + 'x' + col2] = x_test[col1] * x_test[col2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b03b40b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 182)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_x_test.shape #102행, 182개 column을 가진 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b364722e",
   "metadata": {},
   "source": [
    "### 오차 = 예측 - 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b25a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_pre_train = lr2.predict(extended_x_train)\n",
    "ex_pre_test = lr2.predict(extended_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca8ae06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_train_mse = mean_squared_error(ex_pre_train, y_train)\n",
    "ex_test_mse = mean_squared_error(ex_pre_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5a54d8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse : 4.340278052012249\n",
      "test mse : 31.277814972583755\n",
      "train, test mse의 차이 : -26.937536920571507\n"
     ]
    }
   ],
   "source": [
    "print('train mse :', ex_train_mse)\n",
    "print('test mse :', ex_test_mse)\n",
    "print('train, test mse의 차이 :', ex_train_mse - ex_test_mse) #절대값으로 생각하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f2090784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse : 2.0833333991496055\n",
      "test rmse : 5.592657237180172\n",
      "train, test rmse의 차이 : -3.509323838030567\n"
     ]
    }
   ],
   "source": [
    "print('train rmse :', ex_train_mse**0.5)\n",
    "print('test rmse :', ex_test_mse**0.5)\n",
    "print('train, test rmse의 차이 :', ex_train_mse**0.5 - ex_test_mse**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c72982e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9490240966612834"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2.score(extended_x_train, y_train) #설명력: 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a1e67578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6158858583939284"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2.score(extended_x_test, y_test) #설명력: 62%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd79155b",
   "metadata": {},
   "source": [
    "훈련 데이터에만 설명력이 높으니 과대적합의 가능성이 있겠구나.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5aea19",
   "metadata": {},
   "source": [
    "### 라쏘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0c269c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "745feb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha = 10) #alpha: 규제의 강도, alpha값이 높아지면 과대적합이 줄어듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4b294269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.194e+03, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=10)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.fit(extended_x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9dfbb21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용한 특성의 수 : 46\n"
     ]
    }
   ],
   "source": [
    "print('사용한 특성의 수 :', np.sum(lasso.coef_ != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bbccd432",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_lasso = lasso.predict(extended_x_train)\n",
    "pre_test_lasso = lasso.predict(extended_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "61552bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_train_mse = mean_squared_error(pre_train_lasso, y_train)\n",
    "lasso_test_mse = mean_squared_error(pre_test_lasso, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "60a819c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse : 9.825814145224081\n",
      "test mse : 26.404674056715386\n",
      "train, test mse 차이 : -16.578859911491307\n"
     ]
    }
   ],
   "source": [
    "print('train mse :', lasso_train_mse)\n",
    "print('test mse :', lasso_test_mse)\n",
    "print('train, test mse 차이 :', lasso_train_mse - lasso_test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cb6a13de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse : 3.1346154700734954\n",
      "test rmse : 5.138547854862829\n",
      "train, test rmse 차이 : -2.003932384789334\n"
     ]
    }
   ],
   "source": [
    "print('train rmse :', lasso_train_mse**0.5)\n",
    "print('test rmse :', lasso_test_mse**0.5)\n",
    "print('train, test rmse 차이 :', lasso_train_mse**0.5 - lasso_test_mse**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6b9088a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8845973124097618"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.score(extended_x_train, y_train) #r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8017a1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6757315458712968"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.score(extended_x_test, y_test) #r2\n",
    "#훈련데이터의 설명력만 높으므로 과대적합을 의심해 볼 수 있다.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb824755",
   "metadata": {},
   "source": [
    "### 릿지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b0531a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1de6c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3b6b4d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=10)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.fit(extended_x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ff8ec4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_ridge = ridge.predict(extended_x_train)\n",
    "pre_test_ridge = ridge.predict(extended_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "726bd20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_train_mse = mean_squared_error(pre_train_ridge, y_train)\n",
    "ridge_test_mse = mean_squared_error(pre_test_ridge, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "80e9e49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse : 5.09456324639931\n",
      "test mse : 26.79259651669547\n",
      "train, test 차이 : -21.698033270296158\n"
     ]
    }
   ],
   "source": [
    "print('train mse :', ridge_train_mse)\n",
    "print('test mse :', ridge_test_mse)\n",
    "print('train, test 차이 :', ridge_train_mse - ridge_test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e98643c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse : 2.25711391967692\n",
      "test rmse : 5.176156539044725\n",
      "train, test rmse 차이 : -2.919042619367805\n"
     ]
    }
   ],
   "source": [
    "print('train rmse :',ridge_train_mse**0.5)\n",
    "print('test rmse :', ridge_test_mse**0.5)\n",
    "print('train, test rmse 차이 :', ridge_train_mse**0.5 - ridge_test_mse**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fead4182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9401651321668143"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.score(extended_x_train, y_train) #r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "715532c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6709675780923591"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.score(extended_x_test, y_test) #r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ce4d70",
   "metadata": {},
   "source": [
    "같은 alpha 값을 가질 때, 릿지모델보다 랏쏘 모델에서 과대적합을 더 잘 해소한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ed8437",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 튜닝\n",
    "- alpha 값을 바꿔가면서 rmse 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec1a3f3",
   "metadata": {},
   "source": [
    "#### 랏쏘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3d1d96c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "lasso_train_list=[] #train 데이터의 rmse 값을 담아줄 빈 리스트\n",
    "lasso_test_list = [] #test 데이터의 rmse 값을 담아줄 빈 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a8637f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.071e+03, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.188e+03, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.462e+03, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.803e+03, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.194e+03, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.870e+01, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "for i in alpha_list :\n",
    "    lasso = Lasso(alpha = i)\n",
    "    lasso.fit(extended_x_train, y_train)\n",
    "\n",
    "    #train\n",
    "    train_pred = lasso.predict(extended_x_train)\n",
    "    lasso_train_rmse = mean_squared_error(train_pred, y_train)**0.5\n",
    "    lasso_train_list.append(lasso_train_rmse)\n",
    "    \n",
    "    #test\n",
    "    test_pred = lasso.predict(extended_x_test)\n",
    "    lasso_test_rmse = mean_squared_error(test_pred, y_test)**0.5\n",
    "    lasso_test_list.append(lasso_test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7b1040e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.2758518221817816,\n",
       " 2.332152582439725,\n",
       " 2.556464318296377,\n",
       " 2.8421497511641136,\n",
       " 3.1346154700734954,\n",
       " 3.9961466355356023,\n",
       " 5.353949685680336]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "489012e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.795351938140569,\n",
       " 5.447613887368501,\n",
       " 4.676403047411522,\n",
       " 5.053153764089218,\n",
       " 5.138547854862829,\n",
       " 5.737963242918281,\n",
       " 6.985827230756549]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "846851f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d982ed92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(lasso_train_list, label='train')\n",
    "plt.plot(lasso_test_list, label='test')\n",
    "plt.title('Lasso 모델 alpha - RMSE 그래프')\n",
    "plt.xlabel('alpha 값')\n",
    "plt.xticks(np.arange(7),alpha_list) #x축 범위 설정(x축 값의 개수, 바꿔줄 값)\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e73c18",
   "metadata": {},
   "source": [
    "### 릿지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2ffdc165",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "ridge_train_list=[] #train 데이터의 rmse 값을 담아줄 빈 리스트\n",
    "ridge_test_list = [] #test 데이터의 rmse 값을 담아줄 빈 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "647fbd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=4.05867e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "for i in alpha_list :\n",
    "    ridge = Ridge(alpha = i)\n",
    "    ridge.fit(extended_x_train, y_train)\n",
    "\n",
    "    #train\n",
    "    train_pred = ridge.predict(extended_x_train)\n",
    "    ridge_train_rmse = mean_squared_error(train_pred, y_train)**0.5\n",
    "    ridge_train_list.append(ridge_train_rmse)\n",
    "    \n",
    "    #test\n",
    "    test_pred = ridge.predict(extended_x_test)\n",
    "    ridge_test_rmse = mean_squared_error(test_pred, y_test)**0.5\n",
    "    ridge_test_list.append(ridge_test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f2975eb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0835065507717947,\n",
       " 2.0850764409779257,\n",
       " 2.0994432636573346,\n",
       " 2.1580820650757446,\n",
       " 2.25711391967692,\n",
       " 2.4260271435858827,\n",
       " 2.599646390917805]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "781ba809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.653502668471952,\n",
       " 5.874871100553151,\n",
       " 6.0103239800468335,\n",
       " 5.783909816750169,\n",
       " 5.176156539044725,\n",
       " 4.765046455866838,\n",
       " 4.8818074802533555]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "784aef8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(ridge_train_list, label='train')\n",
    "plt.plot(ridge_test_list, label='test')\n",
    "plt.title('Ridge 모델 alpha - RMSE 그래프')\n",
    "plt.xlabel('alpha 값')\n",
    "plt.xticks(np.arange(7),alpha_list) #x축 범위 설정(x축 값의 개수, 바꿔줄 값)\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df881a",
   "metadata": {},
   "source": [
    "데이터에 따라 적합한 모델, alpha값 설정해서 좋은 모듈 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e73988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
